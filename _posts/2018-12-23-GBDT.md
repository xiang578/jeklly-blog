---
layout: post
title: GBDT: Gradient Boosting Decision Tree
tags: [machine learning, tree, gbdt]
category: [Machine Learning]
status: publish
type: post
published: true
---

Gradient Boosting Decision Tree 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 `greedy function approximation ：a gradient boosting machine` 中提出。 工业界有很多成熟的模型是基于 GBDT+FM 开发的，我们老大就认为 GBDT 是传统的机器学习集大成者。之前自己也从网上搜集资料看了很多次，感觉这种方法和其他的方法不同，完全是无从下手。迫于我在元旦后要在组内分享，下定决心重新学一番，在此记录一些看到的东西作为被忘。

## 提升

现实生活中会很多表决决策的时候，他是一种利用群众智慧的方法。对应到机器学习中我们可以训练多个弱的分类器，然后组成一个强分类器来解决问题。从弱分类器到强分类器的过程包括两种方法：提升和集成。

## 梯度提升回归问题

输入：训练数据集 ${T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}}, x_i \in \mathcal{X} \subseteq R^n, y_i \in \mathcal{Y} \subseteq R^n$；损失函数 ${L(y,f(x))}$ 
输出：回归树 ${F_m(x)}$

1. 初始化 
2. 对 m = 
    1. 计算负梯度
    2. 根据 y 拟合出到第 m 棵回归树， 对应的叶结点区域
    3. 


## 分类问题

## 参考文献

1. 李航, 《统计学习方法》8.4 提升树
2. Freidman，greedy function approximation ：a gradient boosting machine
3. 

